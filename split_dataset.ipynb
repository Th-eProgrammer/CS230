{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use smaller training set\n",
    "USE_SMALL_DATA = False\n",
    "SMALL_DATA_CUTOFF = 0.05\n",
    "\n",
    "# split into 3D numpy arrays flag\n",
    "SPLIT_3D = True\n",
    "\n",
    "# training-val-dev split\n",
    "TRAIN_CUTOFF = 0.70\n",
    "VAL_CUTOFF = 0.85\n",
    "\n",
    "# directory pointing to all .npy files\n",
    "input_directory = '../../data/FETAL/processed'\n",
    "\n",
    "# experiment directories\n",
    "output_directory = '../../data/FETAL'\n",
    "\n",
    "assert os.path.isdir(input_directory), \"Couldn't find the dataset at {}\".format(input_directory)\n",
    "assert os.path.isdir(output_directory), \"Couldn't find the dataset at {}\".format(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the array right before it is saved, divides by max value multiplies by 255 and rounds to an int array\n",
    "def normalize_255(a):\n",
    "    a = a * (255.0 / a.max()) if a.max() != 0 else a\n",
    "    a = a.astype(int)\n",
    "    return a\n",
    "\n",
    "def slice_and_save(filename, output_dir):\n",
    "    \"\"\"Slice the 3d numpy array contained in `filename` into 2d numpy arrays and save \n",
    "    it to the `output_dir`\"\"\"\n",
    "    raw_matrix = np.load(filename)\n",
    "    z_slices, width, height = raw_matrix.shape\n",
    "    \n",
    "    # print(filename)\n",
    "    # print(output_dir)\n",
    "    # print()\n",
    "    input_file = os.path.split(filename)[1].split(\".\")[0]\n",
    "    if SPLIT_3D:\n",
    "#         print(z_slices)\n",
    "#         if z_slices == 30:\n",
    "        output_file_name = \"%s.npy\" % (input_file)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        np.save(output_file_path, normalize_255(raw_matrix))\n",
    "    else:\n",
    "        for slice_num, raw_slice in enumerate(raw_matrix):\n",
    "            output_file_name = \"%s_%s.npy\" % (input_file, str(slice_num).zfill(4))\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "            np.save(output_file_path, normalize_255(raw_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3243 [00:00<03:07, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: output dir ../../data/FETAL already exists\n",
      "Warning: dir ../../data/FETAL/train already exists\n",
      "Processing train data, saving preprocessed data to ../../data/FETAL/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2656/3243 [36:38<08:05,  1.21it/s]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not enough free space to write 33554432 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7291b54a63f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing {} data, saving preprocessed data to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mslice_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done building dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-53a6b258003a>\u001b[0m in \u001b[0;36mslice_and_save\u001b[0;34m(filename, output_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s.npy\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_255\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mslice_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 511\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough free space to write 33554432 bytes"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Get the filenames in the input directory\n",
    "    filenames = os.listdir(input_directory)\n",
    "    filenames = [os.path.join(input_directory, f) for f in filenames if f.endswith('.npy')]\n",
    "\n",
    "    # Make sure to always shuffle with a fixed seed so that the split is reproducible\n",
    "    random.seed(230)\n",
    "    filenames.sort()\n",
    "    random.shuffle(filenames)\n",
    "\n",
    "#     # Whether to use a smaller subset\n",
    "#     if USE_SMALL_DATA:\n",
    "#         small_split = int(SMALL_DATA_CUTOFF * len(filenames))\n",
    "#         filenames = filenames[:small_split]\n",
    "\n",
    "#         # Reshuffle the filenames\n",
    "#         filenames.sort()\n",
    "#         random.shuffle(filenames)\n",
    "\n",
    "    # Split the image into 70% train and 15% val and 15% test\n",
    "    first_split = int(TRAIN_CUTOFF * len(filenames))\n",
    "    second_split = int(VAL_CUTOFF * len(filenames))\n",
    "    train_filenames = filenames[:first_split]\n",
    "    val_filenames = filenames[first_split:second_split]\n",
    "    test_filenames = filenames[second_split:]\n",
    "\n",
    "    filenames = {'train': train_filenames,\n",
    "                 'val': val_filenames,\n",
    "                 'test': test_filenames}\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    else:\n",
    "        print(\"Warning: output dir {} already exists\".format(output_directory))\n",
    "\n",
    "    # Preprocess train, val and test\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        output_dir_split = os.path.join(output_directory, '{}'.format(split))\n",
    "        if not os.path.exists(output_dir_split):\n",
    "            os.mkdir(output_dir_split)\n",
    "        else:\n",
    "            print(\"Warning: dir {} already exists\".format(output_dir_split))\n",
    "\n",
    "        print(\"Processing {} data, saving preprocessed data to {}\".format(split, output_dir_split))\n",
    "        for filename in filenames[split]:\n",
    "            slice_and_save(filename, output_dir_split)\n",
    "\n",
    "    print(\"Done building dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
