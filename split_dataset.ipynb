{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from shutil import copyfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use smaller training set\n",
    "USE_SMALL_DATA = False\n",
    "SMALL_DATA_CUTOFF = 0.05\n",
    "\n",
    "# split into 3D numpy arrays flag\n",
    "SPLIT_3D = True\n",
    "\n",
    "# training-val-dev split\n",
    "TRAIN_CUTOFF = 0.70\n",
    "VAL_CUTOFF = 0.85\n",
    "\n",
    "# directory pointing to all .npy files\n",
    "input_directory = '../../data/FETAL/processed'\n",
    "\n",
    "# experiment directories\n",
    "output_directory = '../../data/FETAL'\n",
    "\n",
    "assert os.path.isdir(input_directory), \"Couldn't find the dataset at {}\".format(input_directory)\n",
    "assert os.path.isdir(output_directory), \"Couldn't find the dataset at {}\".format(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the array right before it is saved, divides by max value multiplies by 255 and rounds to an int array\n",
    "def normalize_255(a):\n",
    "    a = a * (255.0 / a.max()) if a.max() != 0 else a\n",
    "    a = a.astype(int)\n",
    "    return a\n",
    "\n",
    "def pad_matrix(matrix, max_z):\n",
    "    z_slices, width, height = matrix.shape\n",
    "    pad_width = max_z - z_slices\n",
    "    matrix = np.pad(matrix, [(0, pad_width), (0, 0), (0, 0)], mode='constant', constant_values=0)\n",
    "    return matrix\n",
    "\n",
    "def slice_and_save(filename, output_dir):\n",
    "    \"\"\"Slice the 3d numpy array contained in `filename` into 2d numpy arrays and save \n",
    "    it to the `output_dir`\"\"\"\n",
    "    raw_matrix = np.load(filename)\n",
    "    z_slices, width, height = raw_matrix.shape\n",
    "    \n",
    "    # print(filename)\n",
    "    # print(output_dir)\n",
    "    # print()\n",
    "    input_file = os.path.split(filename)[1].split(\".\")[0]\n",
    "    if SPLIT_3D:\n",
    "        output_file_name = \"%s.npy\" % (input_file)\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        if z_slices <= 40 and (width == height):\n",
    "#             raw_matrix = normalize_255(raw_matrix)\n",
    "            if z_slices < 40:\n",
    "                raw_matrix = pad_matrix(raw_matrix, 40)\n",
    "            # copyfile(filename, output_file_path)\n",
    "            # copyfile(filename, output_file_path)\n",
    "            np.save(output_file_path, raw_matrix)            \n",
    "    else:\n",
    "        for slice_num, raw_slice in enumerate(raw_matrix):\n",
    "            output_file_name = \"%s_%s.npy\" % (input_file, str(slice_num).zfill(4))\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)\n",
    "            np.save(output_file_path, normalize_255(raw_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3243 [00:00<03:32, 15.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: output dir ../../data/FETAL already exists\n",
      "Warning: dir ../../data/FETAL/train already exists\n",
      "Processing train data, saving preprocessed data to ../../data/FETAL/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3243/3243 [05:18<00:00, 10.19it/s]\n",
      "  0%|          | 2/695 [00:00<00:45, 15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: dir ../../data/FETAL/val already exists\n",
      "Processing val data, saving preprocessed data to ../../data/FETAL/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [02:23<00:00,  4.83it/s]\n",
      "  0%|          | 3/695 [00:00<00:35, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: dir ../../data/FETAL/test already exists\n",
      "Processing test data, saving preprocessed data to ../../data/FETAL/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [02:01<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Get the filenames in the input directory\n",
    "    filenames = os.listdir(input_directory)\n",
    "    filenames = [os.path.join(input_directory, f) for f in filenames if f.endswith('.npy')]\n",
    "\n",
    "    # Make sure to always shuffle with a fixed seed so that the split is reproducible\n",
    "    random.seed(230)\n",
    "    filenames.sort()\n",
    "    random.shuffle(filenames)\n",
    "\n",
    "#     # Whether to use a smaller subset\n",
    "#     if USE_SMALL_DATA:\n",
    "#         small_split = int(SMALL_DATA_CUTOFF * len(filenames))\n",
    "#         filenames = filenames[:small_split]\n",
    "\n",
    "#         # Reshuffle the filenames\n",
    "#         filenames.sort()\n",
    "#         random.shuffle(filenames)\n",
    "\n",
    "    # Split the image into 70% train and 15% val and 15% test\n",
    "    first_split = int(TRAIN_CUTOFF * len(filenames))\n",
    "    second_split = int(VAL_CUTOFF * len(filenames))\n",
    "    train_filenames = filenames[:first_split]\n",
    "    val_filenames = filenames[first_split:second_split]\n",
    "    test_filenames = filenames[second_split:]\n",
    "\n",
    "    filenames = {'train': train_filenames,\n",
    "                 'val': val_filenames,\n",
    "                 'test': test_filenames}\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    else:\n",
    "        print(\"Warning: output dir {} already exists\".format(output_directory))\n",
    "\n",
    "    # Preprocess train, val and test\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        output_dir_split = os.path.join(output_directory, '{}'.format(split))\n",
    "        if not os.path.exists(output_dir_split):\n",
    "            os.mkdir(output_dir_split)\n",
    "        else:\n",
    "            print(\"Warning: dir {} already exists\".format(output_dir_split))\n",
    "\n",
    "        print(\"Processing {} data, saving preprocessed data to {}\".format(split, output_dir_split))\n",
    "        for filename in tqdm(filenames[split]):\n",
    "            slice_and_save(filename, output_dir_split)\n",
    "\n",
    "    print(\"Done building dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
