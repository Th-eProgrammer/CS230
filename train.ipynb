{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import model.net as net\n",
    "import model.data_loader as data_loader\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/home/dxue/CS230/data/FETAL'\n",
    "#data_dir = '/home/sbakr/data'\n",
    "data_dir = '/home/data'\n",
    "model_dir = 'experiments/base_model'\n",
    "restore_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            #import pdb; pdb.set_trace()\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.data[0]\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.data[0])\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n",
    "                       restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n",
    "\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc>=best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the parameters from json file\n",
    "#args = parser.parse_args()\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "\n",
    "  # Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "  # Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "  # Create the input data pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/data/train\n",
      "1_4107.npy\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(data_dir, \"{}\".format('train'))\n",
    "print(path)\n",
    "filenames = os.listdir(path)\n",
    "print(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "- done.\n",
      "Starting training for 10 epoch(s)\n",
      "Epoch 1/10\n",
      "100%|██████████| 11/11 [00:11<00:00,  1.32it/s, loss=106.109]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 87.034\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 1308.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 2/10\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.36it/s, loss=78.438]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 93.560\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 217.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 3/10\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.50it/s, loss=81.941]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 82.726\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 275.462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 4/10\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.39it/s, loss=81.497]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 90.439\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 441.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 5/10\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.75it/s, loss=81.421]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 89.472\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 226.344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 6/10\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.71it/s, loss=85.529]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 96.509\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 179.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 7/10\n",
      "100%|██████████| 11/11 [00:08<00:00,  2.00it/s, loss=80.954]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 81.757\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 254.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 8/10\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.74it/s, loss=86.650]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 118.363\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 106.338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 9/10\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.56it/s, loss=81.910]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 91.140\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 74.227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 10/10\n",
      "100%|██████████| 11/11 [00:08<00:00,  2.10it/s, loss=81.449]\n",
      "- Train metrics: accuracy: 0.000 ; loss: 90.164\n",
      "- Eval metrics : accuracy: 0.000 ; loss: 79.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "  # fetch dataloaders\n",
    "dataloaders = data_loader.fetch_dataloader(['train', 'val'], data_dir, params)\n",
    "train_dl = dataloaders['train']\n",
    "val_dl = dataloaders['val']\n",
    "\n",
    "logging.info(\"- done.\")\n",
    "\n",
    "   # Define the model and optimizer\n",
    "model = net.Net(params).cuda() if params.cuda else net.Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "   # fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics\n",
    "\n",
    "   # Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_dl, val_dl, optimizer, loss_fn, metrics, params, model_dir,\n",
    "                      restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
